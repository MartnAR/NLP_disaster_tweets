{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "# Standard libraries for data processing \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import random \n",
    "import re\n",
    "\n",
    "# Data visualization\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from PIL import Image \n",
    "\n",
    "# NLTK processing \n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "# Modeling \n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "import tensorflow as tf \n",
    "import torch as pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data sets \n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How long is the data set?\n",
    "print(\"Length of the training data set: {}\\n\".format(len(train)))\n",
    "\n",
    "# Select a random sample of the data set\n",
    "train.sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print data types for objects.\n",
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Object types should be converted to string types \n",
    "obj = ['keyword', 'location', 'text']\n",
    "\n",
    "train[obj] = train[obj].astype(str)\n",
    "test[obj] = test[obj].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many items of each target are there?\n",
    "train.groupby('target').id.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate stemmer and lemmatizer from nltk package\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Loop through data frame to lowercase keywords and stem them. \n",
    "for i in range(len(train)): \n",
    "    train.loc[i, 'keyword'] = str(re.sub('%20', ' ', train.loc[i, 'keyword'])) # For words separated by %20, replace with a blank space\n",
    "    train.loc[i, 'keyword_stem'] = ps.stem(train.loc[i, 'keyword'])            # Stem words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes in a data frame and target, returning a keyword pairs dictionary \n",
    "def kwDict(df, target): \n",
    "    kw = list(df[df['target']==target].keyword_stem)\n",
    "    wordfreq = [kw.count(k) for k in kw]\n",
    "    kw_dict = dict(list(zip(kw, wordfreq)))\n",
    "    return kw_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate keyword pairs for both disaster and non-disaster lists\n",
    "kw1_dict = kwDict(train, 1)\n",
    "kw0_dict = kwDict(train, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out first ten\n",
    "print({k: kw1_dict[k] for k in list(kw1_dict)[:10]})\n",
    "print({k: kw0_dict[k] for k in list(kw0_dict)[:10]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word clouds for both kw1 and kw0 dictionaries \n",
    "kw1_wc = WordCloud(background_color=\"black\", width=2500, height=2500, relative_scaling=1.0).generate_from_frequencies(kw1_dict)\n",
    "kw0_wc = WordCloud(background_color=\"black\", width=2500, height=2500, relative_scaling=1.0).generate_from_frequencies(kw0_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build word clouds for both targets\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "# Wordcloud for Target = 1 \n",
    "plt.subplot(121)\n",
    "plt.imshow(kw1_wc)\n",
    "plt.title('Target 1 Wordcloud')\n",
    "\n",
    "# Wordcloud for Target = 0 \n",
    "plt.subplot(122)\n",
    "plt.imshow(kw0_wc)\n",
    "plt.title('Target 0 Wordcloud')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to stem text\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(ps.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function that cleans text \n",
    "def cleanText(df, Series): \n",
    "    # Removes whitespace characters and converts words to lower case \n",
    "    df[Series] = df[Series].map(lambda x: re.sub(r'\\W', ' ', x).lower())\n",
    "    \n",
    "    # Set stopwords \n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend(['co', 'http', 'https', 'รป_', 'via'])\n",
    "    stop_words = set(stop_words)\n",
    "    \n",
    "    # Tokenize text \n",
    "    df['word_tokens'] = df.apply(lambda row: word_tokenize(row[Series]), axis=1)\n",
    "\n",
    "    # Remove stopwords to filter out noise and reconnect strings into sentences. \n",
    "    df['clean_text'] = df['word_tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "    df['clean_text'] = df['clean_text'].apply(lambda x: ' '.join(str(word) for word in x))\n",
    "\n",
    "    # Stem cleaned text using stemSentence function \n",
    "    df['stemmed'] = df['clean_text'].map(lambda x: stemSentence(x))\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(vectorizer):\n",
    "    vec = vectorizer.fit(train['stemmed'])\n",
    "    return vec\n",
    "\n",
    "# Function that vectorizes training and testing sets with either tf-idf or count vectorizer\n",
    "def featureVec(vectorizer, df):\n",
    "    vec_df = pd.DataFrame(vectorizer.transform(df['stemmed']).todense(), columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    if len(df.columns) > 1: \n",
    "        add_df = df.reset_index()\n",
    "        X_df = pd.concat([vec_df, add_df.iloc[:, 2:]], axis=1)\n",
    "    else: \n",
    "        X_df = vec_df\n",
    "    \n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelRun(model, param_grid): \n",
    "    random.seed(42)\n",
    "    mod_cv = GridSearchCV(model, param_grid=param_grid, cv=10, n_jobs=-1, verbose=0)\n",
    "    mod_cv.fit(X_train, y_train)\n",
    "    \n",
    "    # Print the tuned parameters and score \n",
    "    print(\"Tuned Logistic Regression Parameters: {}\".format(mod_cv.best_params_))\n",
    "    print(\"Best score is {}\".format(round(100 * mod_cv.best_score_, 2)))\n",
    "    \n",
    "    # Run model with best hyperparametres; print confusion matrix. \n",
    "    y_pred = mod_cv.predict(X_test)\n",
    "    print('F1-score: {}\\n'.format(round(100 * f1_score(y_test, y_pred), 3)))\n",
    "    print('Confusion matrix:\\n {}'.format(confusion_matrix(y_test, y_pred)))\n",
    "    \n",
    "    return mod_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = cleanText(train, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoder \n",
    "le = LabelEncoder()\n",
    "kw = le.fit(train['keyword_stem'])\n",
    "train['kws_le'] = kw.transform(train['keyword_stem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model features, target, and train_test_split\n",
    "features = ['stemmed']\n",
    "target = ['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train[features], train[target], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectorizer for data modeling \n",
    "cv = CountVectorizer(min_df=2, binary=True, encoding='utf-8', ngram_range=(1,2), stop_words='english')\n",
    "\n",
    "# TF-IDF Vectorizer for data modeling \n",
    "tf = TfidfVectorizer(min_df=2, norm='l2', encoding='utf-8', ngram_range=(1,2), stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = vectorize(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = featureVec(vec, X_train)\n",
    "X_test = featureVec(vec, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with hyperparameters for tuning \n",
    "iterations = range(20000, 23001, 1000)\n",
    "c_space = np.arange(0.1, 0.5, 0.1)\n",
    "alpha_fit = np.arange(0.1, 1.1, 0.1)\n",
    "\n",
    "svc_param_grid = {'max_iter':iterations, \n",
    "             'C':c_space}\n",
    "\n",
    "mnb_param_grid = {'alpha':alpha_fit}\n",
    "\n",
    "svc = LinearSVC(random_state=42)\n",
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = modelRun(mnb, mnb_param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next steps:\n",
    "# 1. SIA for text\n",
    "# 2. Length of text, word count, avg word length\n",
    "# 3. Test pytorch and tensorflow NN. \n",
    "\n",
    "# Bonus: \n",
    "# 1. Work with pipelines \n",
    "# 2. Clean locations\n",
    "# 3. Tie locations to keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test)): \n",
    "    test.loc[i, 'keyword'] = str(re.sub('%20', ' ', test.loc[i, 'keyword'])) # For words separated by %20, replace with a blank space\n",
    "    test.loc[i, 'keyword_stem'] = ps.stem(test.loc[i, 'keyword'])            # Stem words\n",
    "    \n",
    "test['kws_le'] = kw.transform(test['keyword_stem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = cleanText(test, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = featureVec(vec, test[features])\n",
    "#df_test = pd.concat([df_test, test['kws_le'].reset_index()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 7\n",
    "name = 'results_v'+str(version)+'.csv'\n",
    "\n",
    "results = pd.concat([test['id'], pd.DataFrame(test_pred)], axis=1)\n",
    "results.rename(columns={0:'target'}, inplace=True)\n",
    "results.to_csv(name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
